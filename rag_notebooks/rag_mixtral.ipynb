{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trung\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import OpenCLIPEmbeddingFunction\n",
    "from chromadb.utils.data_loaders import ImageLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = chromadb.PersistentClient(path=\"chromadb\")\n",
    "apparel_collection = client.get_or_create_collection(\n",
    "    \"apparel_50k\",\n",
    "    embedding_function=OpenCLIPEmbeddingFunction(),\n",
    "    data_loader=ImageLoader(),\n",
    ")\n",
    "apparel_collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"apparel_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_state = 329\n",
    "test_df = (\n",
    "    df.sample(n=len(df), random_state=random_state).groupby(\"category_id\").head(25)\n",
    ")\n",
    "test_prices = test_df[\"price\"].values\n",
    "test_descriptions = test_df[\"title\"].values\n",
    "\n",
    "len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms.huggingface_endpoint import HuggingFaceEndpoint\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_experimental.open_clip import OpenCLIPEmbeddings\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Setting up LANGCHAIN_API_KEY, HUGGINGFACEHUB_API_TOKEN\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"price-discovery\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\trung\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"mistralai/Mixtral-8x7B-Instruct-v0.1\", model_kwargs={\"use_cache\": False}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_apparel_collection = Chroma(\n",
    "    client=client,\n",
    "    collection_name=\"apparel_50k\",\n",
    "    embedding_function=OpenCLIPEmbeddings(\n",
    "        model_name=\"ViT-B-32\", checkpoint=\"laion2b_s34b_b79k\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "retriever = langchain_apparel_collection.as_retriever(search_kwargs={\"k\": 20})\n",
    "\n",
    "contexts = []\n",
    "\n",
    "\n",
    "def get_similar_products(docs):\n",
    "    simlar_products = \"\"\n",
    "    context = []\n",
    "    for doc in docs[:5]:\n",
    "        simlar_products += \"-{}. Price: ${}\\n\".format(\n",
    "            doc.metadata[\"description\"], doc.metadata[\"price\"]\n",
    "        )\n",
    "        context.append(doc.metadata[\"description\"])\n",
    "    contexts.append(context)\n",
    "    return simlar_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever.get_relevant_documents(query='Men\\'s Retro 6\"Hare Neutral Grey/Black-White (CT8529 062)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "from langchain.output_parsers import OutputFixingParser\n",
    "\n",
    "min_price_schema = ResponseSchema(\n",
    "    name=\"min_price\",\n",
    "    description=\"The reasonable minimum price for the product\",\n",
    "    type=\"number\",\n",
    ")\n",
    "max_price_schema = ResponseSchema(\n",
    "    name=\"max_price\",\n",
    "    description=\"The reasonable maximum price for the product\",\n",
    "    type=\"number\",\n",
    ")\n",
    "reason_schema = ResponseSchema(\n",
    "    name=\"reason\",\n",
    "    description=\"Explanation for establishing the price range\",\n",
    "    type=\"text\",\n",
    ")\n",
    "\n",
    "price_range_parser = StructuredOutputParser.from_response_schemas(\n",
    "    [min_price_schema, max_price_schema, reason_schema]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_range_prompt_template = \"\"\"<s> [INST]\n",
    "Imagine you are an expert in the field of apparel and you are asked to provide a price range for the following product: {original_product}\n",
    "Here are some products that are related to the product you are asked to provide a price range for. \\\n",
    "Pick the most similar products and use them to come up with an accurate price range. \\\n",
    "Similar products are those with closely matching specifications based on criteria such as type of product, functionality, target users, style, material, and brand.\n",
    "\n",
    "SIMILAR PRODUCTS:\n",
    "{similar_products}\n",
    "\n",
    "Please provide a price range for the product you are asked to provide a price range for and a comprehensive and detailed rationale for the specified price range. \\\n",
    "Don't put any comments in the final answer.\n",
    "{format_instructions} [/INST] </s>\n",
    "\"\"\"\n",
    "\n",
    "price_range_prompt = PromptTemplate.from_template(\n",
    "    price_range_prompt_template,\n",
    "    partial_variables={\n",
    "        \"format_instructions\": price_range_parser.get_format_instructions()\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub.utils import HfHubHTTPError\n",
    "from langchain_core.exceptions import OutputParserException\n",
    "import time\n",
    "\n",
    "\n",
    "def hf_call(\n",
    "    chain, input={}, parser=None, max_tries=5\n",
    "):  # input: to be invoked (usualy a dictionary)\n",
    "    i = 0\n",
    "    while i < max_tries:\n",
    "        try:\n",
    "            llm_response = chain.invoke(input)\n",
    "            if parser is None:\n",
    "                return llm_response\n",
    "            parsed_response = parser.parse(llm_response)\n",
    "            return parsed_response\n",
    "        except OutputParserException as e:\n",
    "            print(e)\n",
    "            print(\"Trying to fix the response format\")\n",
    "            try:\n",
    "                output_fixing_prompt = PromptTemplate(\n",
    "                    input_variables=[\"completion\", \"error\", \"instructions\"],\n",
    "                    template=\"<s> [INST] Instructions:\\n--------------\\n{instructions}\\n--------------\\nCompletion:\\n--------------\\n{completion}\\n--------------\\n\\nAbove, the Completion did not satisfy the constraints given in the Instructions.\\nError:\\n--------------\\n{error}\\n--------------\\n\\nPlease try again. Please only respond with an answer that satisfies the constraints laid out in the Instructions: [/INST] </s>\",\n",
    "                )\n",
    "                output_fixing_parser = OutputFixingParser.from_llm(\n",
    "                    parser=parser, prompt=output_fixing_prompt, llm=llm\n",
    "                )\n",
    "                parsed_response = output_fixing_parser.parse(llm_response)\n",
    "                return parsed_response\n",
    "            except OutputParserException as e:\n",
    "                print(e)\n",
    "                print(\"Failed to fix the response format. Will send another LLM call\")\n",
    "                del contexts[-1]\n",
    "                i += 1\n",
    "                continue\n",
    "        except HfHubHTTPError as e:\n",
    "            print(e)\n",
    "            print(f\"Slepping for 1 hour since {time.ctime()}\")\n",
    "            time.sleep(3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\n",
    "        \"similar_products\": retriever | get_similar_products,\n",
    "        \"original_product\": RunnablePassthrough(),\n",
    "    }\n",
    "    | price_range_prompt\n",
    "    | llm\n",
    ")\n",
    "contexts = []\n",
    "price_ranges = []\n",
    "reasons = []\n",
    "for i in range(len(test_descriptions)):\n",
    "    test_description = test_descriptions[i]\n",
    "    test_price = test_prices[i]\n",
    "\n",
    "    print(i)\n",
    "    print(\"Test Description: \", test_description)\n",
    "    print(\"Test Price: $\", test_price)\n",
    "\n",
    "    parsed_response = hf_call(\n",
    "        rag_chain, input=test_description, parser=price_range_parser\n",
    "    )\n",
    "\n",
    "    price_ranges.append([parsed_response[\"min_price\"], parsed_response[\"max_price\"]])\n",
    "    reasons.append(parsed_response[\"reason\"])\n",
    "    print(parsed_response, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_mixtral_dataset = {\n",
    "    \"test_descriptions\": list(test_descriptions),\n",
    "    \"test_prices\": list(test_prices),\n",
    "    \"price_ranges\": price_ranges,\n",
    "    \"reasons\": reasons,\n",
    "    \"contexts\": contexts,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('rag_eval/rag_mixtral_dataset_250.pkl', 'wb') as f:\n",
    "#     pickle.dump(rag_mixtral_dataset, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering the retrieved products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "# from langchain.output_parsers import PydanticOutputParser\n",
    "# from langchain.output_parsers import OutputFixingParser\n",
    "\n",
    "# class IsSimilar(BaseModel):\n",
    "#     similar: bool = Field(..., title=\"Is Similar\", description=\"Whether the two products are similar to each other (True or False)\")\n",
    "\n",
    "# filter_parser = PydanticOutputParser(pydantic_object=IsSimilar)\n",
    "\n",
    "# output_fixing_prompt = PromptTemplate(input_variables=['completion', 'error', 'instructions'], template='<s> [INST] Instructions:\\n--------------\\n{instructions}\\n--------------\\nCompletion:\\n--------------\\n{completion}\\n--------------\\n\\nAbove, the Completion did not satisfy the constraints given in the Instructions.\\nError:\\n--------------\\n{error}\\n--------------\\n\\nPlease try again. Please only respond with an answer that satisfies the constraints laid out in the Instructions: [/INST] </s>')\n",
    "# filter_output_fixing_parser = OutputFixingParser.from_llm(parser=filter_parser, prompt=output_fixing_prompt, llm=llm, max_retries=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter_prompt_template = \"\"\"<s> [INST]\n",
    "#     Imagine you are a apparel retailer and you want to know if the two products are in the same type of apparel category.\\\n",
    "#     There are a few criteria that need to satisfied in order for two products to be in the same category.\n",
    "#     - Same type of products (e.g. both are T-shirts, both are watches)\n",
    "#     - Same functionality: Consider the primary use of the product (e.g. running shoes, casual shoes)\n",
    "#     - Same target users: Consider gender, age group, etc.\n",
    "#     - Similar style: Consider the design, color, etc.\n",
    "#     - Same material: Consider the material used in the product\n",
    "#     - Same brand: Consider the brand name\n",
    "#     You should evaluate each of these criteria and decide if the two products are in the same category.\n",
    "#     Some of these criteria may not be available in the product description. In that case, you can make your own judgement based on the available information.\\\n",
    "#     These are just a few examples of the criteria. You can use your own judgement to decide if the two products are in the same category.\n",
    "#     These are the two products' descriptions:\n",
    "#     Product 1: {product_1}\n",
    "#     Product 2: {product_2}\n",
    "\n",
    "#     Please evaluate if the two products are in the same category(True or False). Make sure to put the result in between three backticks.\n",
    "#     {format_instructions} [/INST] </s>\n",
    "#     \"\"\"\n",
    "# filter_prompt = PromptTemplate.from_template(filter_prompt_template, partial_variables={\"format_instructions\": filter_parser.get_format_instructions()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter_prompt.invoke({\"product_1\": \"This is a T-shirt\", \"product_2\": \"This is a T-shirt\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "# filter_chain = (\n",
    "#     filter_prompt\n",
    "#     | llm\n",
    "# )\n",
    "\n",
    "# llm_responses = {}\n",
    "# not_enough = 0\n",
    "# num_true = 0\n",
    "# num_false = 0\n",
    "# false_products = []\n",
    "\n",
    "# filtered_prices = []\n",
    "# filtered_descriptions = []\n",
    "# for i in range(len(retrieved_descriptions)):\n",
    "#     filtered_price_lst = []\n",
    "#     filtered_description_lst = []\n",
    "#     for j in range(len(retrieved_descriptions[i])):\n",
    "#         print(f\"{i}-{j}. {test_descriptions[i]} vs {retrieved_descriptions[i][j]}\")\n",
    "\n",
    "#         try:\n",
    "#             llm_response = filter_chain.invoke({\"product_1\": test_descriptions[i], \"product_2\": retrieved_descriptions[i][j]})\n",
    "#         except Exception as e:\n",
    "#             print(e)\n",
    "#             print(f\"Slepping for 1 hour since {time.ctime()}\")\n",
    "#             time.sleep(3600)\n",
    "#             llm_response = filter_chain.invoke({\"product_1\": test_descriptions[i], \"product_2\": retrieved_descriptions[i][j]})\n",
    "\n",
    "#         try:\n",
    "#             parsed_response = filter_parser.parse(llm_response)\n",
    "#         except Exception as e:\n",
    "#             print(e)\n",
    "#             print(\"Try to fix output...\")\n",
    "#             parsed_response = filter_output_fixing_parser.parse(llm_response)\n",
    "\n",
    "#         print(parsed_response)\n",
    "\n",
    "#         if parsed_response.similar:\n",
    "#             num_true += 1\n",
    "#             filtered_price_lst.append(retrieved_prices[i][j])\n",
    "#             filtered_description_lst.append(retrieved_descriptions[i][j])\n",
    "#         else:\n",
    "#             num_false += 1\n",
    "#             false_products.append(f\"{i}-{j}. {test_descriptions[i]} vs {retrieved_descriptions[i][j]}\")\n",
    "\n",
    "#         if len(filtered_price_lst) == 5:\n",
    "#             break\n",
    "\n",
    "#     if len(filtered_price_lst) < 3:\n",
    "#         not_enough += 1\n",
    "#         filtered_price_lst = retrieved_prices[i][:5]\n",
    "#         filtered_description_lst = retrieved_descriptions[i][:5]\n",
    "\n",
    "#     filtered_prices.append(filtered_price_lst)\n",
    "#     filtered_descriptions.append(filtered_description_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Not enough:\", not_enough)\n",
    "# print(\"True:\", num_true)\n",
    "# print(\"False:\", num_false)\n",
    "# for product in false_products:\n",
    "#     print(product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filterd_similar_products = []\n",
    "# for i in range(size):\n",
    "#     similar_product = \"\"\n",
    "#     for j in range(len(filtered_descriptions[i])):\n",
    "#         similar_product += \"-{}. Price: ${}\\n\".format(filtered_descriptions[i][j], filtered_prices[i][j])\n",
    "#     filterd_similar_products.append(similar_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter_rag_chain = (\n",
    "#     price_range_prompt\n",
    "#     | llm\n",
    "# )\n",
    "\n",
    "# filter_responses = []\n",
    "# for i in range(size):\n",
    "#     test_description = test_descriptions[i]\n",
    "#     test_price = test_prices[i]\n",
    "#     similar_products = filterd_similar_products[i]\n",
    "\n",
    "#     print(\"Test Description: \", test_description)\n",
    "#     print(\"Test Price: $\", test_price)\n",
    "\n",
    "#     llm_response = filter_rag_chain.invoke({\"similar_products\": similar_products, \"original_product\": test_description})\n",
    "\n",
    "#     try:\n",
    "#         parsed_response = parser.parse(llm_response)\n",
    "#     except Exception as e:\n",
    "#         print(e)\n",
    "#         print(\"Try to fix output...\")\n",
    "#         parsed_response = output_fixing_parser.parse(llm_response)\n",
    "\n",
    "#     filter_responses.append(parsed_response)\n",
    "#     print(parsed_response, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter_rag_dataset = {\n",
    "#     \"test_descriptions\": test_descriptions,\n",
    "#     \"test_prices\": test_prices,\n",
    "#     \"contexts\": filtered_descriptions,\n",
    "#     \"responses\": filter_responses\n",
    "# }\n",
    "# # Save the data to pickle\n",
    "# import pickle\n",
    "# with open('rag_eval/filter_rag_dataset.pkl', 'wb') as f:\n",
    "#     pickle.dump(filter_rag_dataset, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
